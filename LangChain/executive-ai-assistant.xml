<?xml version="1.0"?>
<Container version="2">
  <Name>Executive-AI-Assistant</Name>
  <Repository>ghcr.io/ryan-haver/executive-ai-assistant:feature-multi-llm-support</Repository>
  <Registry>https://github.com/ryan-haver/executive-ai-assistant/pkgs/container/executive-ai-assistant</Registry>
  <Network>bridge</Network>
  <MyIP/>
  <Shell>bash</Shell>
  <Privileged>false</Privileged>
  <Support>https://github.com/ryan-haver/executive-ai-assistant</Support>
  <Project>https://github.com/langchain-ai/executive-ai-assistant</Project>
  <Overview>Executive AI Assistant - Your AI-powered email management assistant that drafts professional responses using LangChain and multiple LLM providers.

This intelligent assistant monitors your Gmail inbox, triages emails, drafts professional responses, and helps you manage your communications efficiently using state-of-the-art AI models.

**KEY FEATURES:**
- üìß Automated email monitoring and triage
- ‚úçÔ∏è AI-powered draft response generation
- üîÑ Multi-LLM support (Ollama, OpenAI, Anthropic)
- üéØ Context-aware response generation
- üìÖ Meeting scheduling assistance
- üîç Intelligent email categorization
- üíæ Persistent conversation history
- üîí Secure OAuth2 Gmail integration

**SUPPORTED LLM PROVIDERS:**
- **Ollama** - Free, local AI models (llama3.2, llama3.1, etc.)
- **OpenAI** - GPT-4o, GPT-4o-mini, O1
- **Anthropic** - Claude 3.5 Sonnet, Claude 3 Haiku
- **Hybrid Mode** - Ollama with cloud fallback (recommended)
- **Auto Mode** - Automatically detect available providers

**IMPORTANT - GMAIL OAUTH SETUP (REQUIRED):**

This container monitors your Gmail and drafts responses. It requires one-time OAuth setup:

**ÔøΩ QUICK SETUP (5 MINUTES):**

**Part 1: Google Cloud Console (One-time)**
1. Visit https://console.cloud.google.com/
2. Create new project: "Executive AI Assistant"
3. Enable Gmail API (APIs &amp; Services ‚Üí Enable APIs)
4. Configure OAuth consent screen:
   - User type: **External**
   - App name: "Executive AI Assistant"
   - Add your Gmail as test user
5. Create credentials (APIs &amp; Services ‚Üí Credentials):
   - Type: **Desktop app**
   - Name: "Executive AI Assistant"
   - Download JSON as `client_secret.json`

**Part 2: Container Setup (One-time)**
1. Deploy this container with at least ONE LLM provider configured (see LLM SETUP below)
2. From your PC, open PowerShell and run:
   ```
   ssh -L 2025:localhost:2025 root@YOUR-UNRAID-IP
   ```
   (Replace YOUR-UNRAID-IP with your server's IP like 192.168.1.110)
3. Open http://localhost:2025/setup in your browser
4. Upload `client_secret.json` 
5. Click "Connect to Gmail" and authorize
6. Done! Close PowerShell. Container now runs independently.

**‚ö†Ô∏è SSH tunnel is ONLY for initial setup** - the container runs on its own afterwards, checking email every 5 minutes automatically.

---

**LLM SETUP (REQUIRED - Choose One):**

You must configure at least ONE AI provider:

**Option A: Ollama (Free, Local)**
- Deploy Ollama container from Community Apps
- Run: `docker exec ollama ollama pull llama3.2:3b`
- Set "Ollama Host" to: `http://YOUR-UNRAID-IP:11434`
- Set "LLM Provider" to: `ollama` or `auto`

**Option B: OpenAI (Cloud, Paid)**
- Get API key: https://platform.openai.com/api-keys
- Paste in "OpenAI API Key" field
- Set "LLM Provider" to: `openai` or `auto`

**Option C: Anthropic (Cloud, Paid)**
- Get API key: https://console.anthropic.com/settings/keys
- Paste in "Anthropic API Key" field  
- Set "LLM Provider" to: `anthropic` or `auto`

**üí° Recommended:** Set "LLM Provider" to `auto` - it will use whichever provider you configured.

---

**ACCESSING THE ASSISTANT:**

- **Setup UI:** http://YOUR-UNRAID-IP:2025/setup (for OAuth setup only)
- **LangGraph API:** http://YOUR-UNRAID-IP:2024 (for LangGraph Studio)
- **Agent Inbox UI:** Install separate Agent-Inbox container for web interface

---

**ADVANCED OPTIONS:**

**Custom Models:** In "Show more settings", you can customize which models are used for each task (triage, drafting, etc.)

**Reverse Proxy:** If you have a public domain:
- Set USE_LOCALHOST_OAUTH to `false`
- Use Web app OAuth credentials instead of Desktop app
- Set SETUP_UI_BASE_URL to your public URL

**Troubleshooting:** Check container logs for errors. Most issues are due to missing LLM provider or incorrect OAuth setup.

**Container won't start / Health check failing:**
- Check logs: `docker logs executive-ai-assistant`
- Verify Gmail credentials are base64-encoded correctly
- Ensure at least one LLM provider is configured

**Gmail OAuth errors:**
- Re-run setup_gmail.py script to regenerate token
- Verify OAuth consent screen is configured
- Check that test users include your email
- Make sure Gmail API is enabled

**Ollama connection failed:**
- Verify Ollama is running: `docker ps | grep ollama`
- Test connection: `curl http://YOUR_UNRAID_IP:11434/api/tags`
- Check Ollama Host setting matches your setup
- Ensure network allows container-to-container communication

**No emails being processed:**
- Check cron logs: `docker exec executive-ai-assistant cat /var/log/eaia/cron.log`
- Verify email arrived in last X minutes (CRON_MINUTES_SINCE setting)
- Check Gmail token hasn't expired
- Review API logs for errors

**SECURITY NOTES:**
- Gmail credentials are stored in Docker volumes (not visible in Unraid GUI)
- API keys are masked in the template (not shown in plain text)
- Consider using Ollama for privacy-sensitive emails
- LangSmith tracing is optional (disabled by default)
- Keep your OAuth credentials secure and don't share them

**RESOURCE REQUIREMENTS:**
- CPU: 0.5-2.0 cores (depending on LLM usage)
- RAM: 512MB-2GB (depending on model size)
- Disk: ~1GB for container + email data
- Network: Internet access for cloud LLMs (optional for Ollama-only)

**DOCKER IMAGE DETAILS:**
- Base: python:3.12-slim
- Size: 693MB
- Services: LangGraph API (port 2024) + Setup UI (port 2025) + Cron scheduler
- Health Check: Automatic monitoring of all services

For detailed documentation, visit: https://github.com/ryan-haver/executive-ai-assistant</Overview>
  <Category>Productivity: Tools:</Category>
  <WebUI>http://[IP]:[PORT:2025]/setup</WebUI>
  <TemplateURL>https://raw.githubusercontent.com/ryan-haver/unraid-docker-templates/main/LangChain/executive-ai-assistant.xml</TemplateURL>
  <Icon>https://python.langchain.com/img/brand/wordmark.png</Icon>
  <ExtraParams>--restart=unless-stopped</ExtraParams>
  <PostArgs/>
  <CPUset/>
  <DateInstalled/>
  <DonateText/>
  <DonateLink/>
  <Description>AI-powered email assistant that monitors Gmail, triages emails, and drafts professional responses using LangChain and multiple LLM providers (Ollama, OpenAI, Anthropic).</Description>
  <Networking>
    <Mode>bridge</Mode>
    <Publish>
      <Port>
        <HostPort>2024</HostPort>
        <ContainerPort>2024</ContainerPort>
        <Protocol>tcp</Protocol>
      </Port>
    </Publish>
  </Networking>
  <Data>
    <Volume>
      <HostDir>/mnt/user/appdata/executive-ai-assistant/data</HostDir>
      <ContainerDir>/app/data</ContainerDir>
      <Mode>rw</Mode>
    </Volume>
    <Volume>
      <HostDir>/mnt/user/appdata/executive-ai-assistant/config</HostDir>
      <ContainerDir>/app/config</ContainerDir>
      <Mode>rw</Mode>
    </Volume>
    <Volume>
      <HostDir>/mnt/user/appdata/executive-ai-assistant/secrets</HostDir>
      <ContainerDir>/app/secrets</ContainerDir>
      <Mode>rw</Mode>
    </Volume>
    <Volume>
      <HostDir>/mnt/user/appdata/executive-ai-assistant/logs</HostDir>
      <ContainerDir>/var/log/eaia</ContainerDir>
      <Mode>rw</Mode>
    </Volume>
  </Data>
  <Labels/>
  
  <!-- Required Configuration -->
  <Config Name="LangGraph API Port" Target="2024" Default="2024" Mode="tcp" Description="Port for LangGraph API and web interface" Type="Port" Display="always" Required="true" Mask="false">2024</Config>
  
  <Config Name="Setup UI Port" Target="2025" Default="2025" Mode="tcp" Description="Port for OAuth Setup Web UI - Use this to configure Gmail credentials via web interface instead of manual setup" Type="Port" Display="always" Required="true" Mask="false">2025</Config>
  
  <Config Name="Gmail Client Secret (Base64)" Target="GMAIL_SECRET" Default="" Mode="" Description="Base64-encoded client_secret.json from Google Cloud Console (OPTIONAL with Setup UI - can configure via web interface at port 2025)" Type="Variable" Display="always" Required="false" Mask="true"/>
  
  <Config Name="Gmail OAuth Token (Base64)" Target="GMAIL_TOKEN" Default="" Mode="" Description="Base64-encoded token.json generated by setup_gmail.py script (OPTIONAL with Setup UI - can configure via web interface at port 2025)" Type="Variable" Display="always" Required="false" Mask="true"/>
  
  <!-- LLM Provider Configuration -->
  <Config Name="LLM Provider" Target="LLM_PROVIDER" Default="auto" Mode="" Description="Choose your AI model provider: 'ollama' (free, local), 'openai' (cloud, paid), 'anthropic' (cloud, paid), 'hybrid' (ollama + cloud fallback), 'auto' (detect available)" Type="Variable" Display="always" Required="true" Mask="false">auto</Config>
  
  <Config Name="Ollama Host" Target="OLLAMA_HOST" Default="http://192.168.1.100:11434" Mode="" Description="Ollama server URL (if using Ollama). Use your Unraid IP or container name. Default port is 11434. Example: http://192.168.1.100:11434 or http://ollama:11434" Type="Variable" Display="always" Required="false" Mask="false">http://192.168.1.100:11434</Config>
  
  <Config Name="OpenAI API Key" Target="OPENAI_API_KEY" Default="" Mode="" Description="OpenAI API key (if using OpenAI). Get from: https://platform.openai.com/api-keys. Format: sk-proj-..." Type="Variable" Display="always" Required="false" Mask="true"/>
  
  <Config Name="Anthropic API Key" Target="ANTHROPIC_API_KEY" Default="" Mode="" Description="Anthropic API key (if using Claude). Get from: https://console.anthropic.com/settings/keys. Format: sk-ant-..." Type="Variable" Display="always" Required="false" Mask="true"/>
  
  <!-- Ollama Model Configuration -->
  <Config Name="Ollama Triage Model" Target="OLLAMA_MODEL_TRIAGE" Default="llama3.2:3b" Mode="" Description="Ollama model for email triage/classification. Default: llama3.2:3b (fast, lightweight). Other options: llama3.1:8b, qwen2.5:3b" Type="Variable" Display="advanced" Required="false" Mask="false">llama3.2:3b</Config>
  
  <Config Name="Ollama Draft Model" Target="OLLAMA_MODEL_DRAFT" Default="llama3.1:8b" Mode="" Description="Ollama model for drafting email responses. Default: llama3.1:8b (balanced). Other options: llama3.1:70b (better quality), llama3.2:3b (faster)" Type="Variable" Display="advanced" Required="false" Mask="false">llama3.1:8b</Config>
  
  <Config Name="Ollama Rewrite Model" Target="OLLAMA_MODEL_REWRITE" Default="llama3.1:8b" Mode="" Description="Ollama model for rewriting/tone adjustment. Default: llama3.1:8b. Other options: llama3.1:70b (better), llama3.2:3b (faster)" Type="Variable" Display="advanced" Required="false" Mask="false">llama3.1:8b</Config>
  
  <Config Name="Ollama Schedule Model" Target="OLLAMA_MODEL_SCHEDULE" Default="llama3.1:8b" Mode="" Description="Ollama model for calendar/scheduling logic. Default: llama3.1:8b. Other options: llama3.1:70b (better reasoning)" Type="Variable" Display="advanced" Required="false" Mask="false">llama3.1:8b</Config>
  
  <Config Name="Ollama Reflect Model" Target="OLLAMA_MODEL_REFLECT" Default="llama3.1:70b" Mode="" Description="Ollama model for reflection/learning. Default: llama3.1:70b (high quality). Warning: 70b models require significant RAM (40GB+)" Type="Variable" Display="advanced" Required="false" Mask="false">llama3.1:70b</Config>
  
  <!-- OpenAI Model Configuration -->
  <Config Name="OpenAI Triage Model" Target="OPENAI_MODEL_TRIAGE" Default="gpt-4o-mini" Mode="" Description="OpenAI model for triage. Default: gpt-4o-mini (fast, cheap). Other options: gpt-4o, gpt-3.5-turbo" Type="Variable" Display="advanced" Required="false" Mask="false">gpt-4o-mini</Config>
  
  <Config Name="OpenAI Draft Model" Target="OPENAI_MODEL_DRAFT" Default="gpt-4o" Mode="" Description="OpenAI model for drafting. Default: gpt-4o (high quality). Other options: gpt-4o-mini (cheaper), o1-mini" Type="Variable" Display="advanced" Required="false" Mask="false">gpt-4o</Config>
  
  <Config Name="OpenAI Rewrite Model" Target="OPENAI_MODEL_REWRITE" Default="gpt-4o" Mode="" Description="OpenAI model for rewriting. Default: gpt-4o. Other options: gpt-4o-mini" Type="Variable" Display="advanced" Required="false" Mask="false">gpt-4o</Config>
  
  <Config Name="OpenAI Schedule Model" Target="OPENAI_MODEL_SCHEDULE" Default="gpt-4o" Mode="" Description="OpenAI model for scheduling. Default: gpt-4o. Other options: o1, o1-mini (better reasoning)" Type="Variable" Display="advanced" Required="false" Mask="false">gpt-4o</Config>
  
  <Config Name="OpenAI Reflect Model" Target="OPENAI_MODEL_REFLECT" Default="o1" Mode="" Description="OpenAI model for reflection. Default: o1 (best reasoning). Other options: gpt-4o, o1-mini" Type="Variable" Display="advanced" Required="false" Mask="false">o1</Config>
  
  <!-- Anthropic Model Configuration -->
  <Config Name="Anthropic Triage Model" Target="ANTHROPIC_MODEL_TRIAGE" Default="claude-3-haiku-20240307" Mode="" Description="Anthropic model for triage. Default: claude-3-haiku (fast). Other options: claude-3-5-sonnet-latest" Type="Variable" Display="advanced" Required="false" Mask="false">claude-3-haiku-20240307</Config>
  
  <Config Name="Anthropic Draft Model" Target="ANTHROPIC_MODEL_DRAFT" Default="claude-3-5-sonnet-latest" Mode="" Description="Anthropic model for drafting. Default: claude-3-5-sonnet-latest (high quality). Other options: claude-3-haiku-20240307 (cheaper)" Type="Variable" Display="advanced" Required="false" Mask="false">claude-3-5-sonnet-latest</Config>
  
  <Config Name="Anthropic Rewrite Model" Target="ANTHROPIC_MODEL_REWRITE" Default="claude-3-5-sonnet-latest" Mode="" Description="Anthropic model for rewriting. Default: claude-3-5-sonnet-latest. Other options: claude-3-haiku-20240307" Type="Variable" Display="advanced" Required="false" Mask="false">claude-3-5-sonnet-latest</Config>
  
  <Config Name="Anthropic Schedule Model" Target="ANTHROPIC_MODEL_SCHEDULE" Default="claude-3-5-sonnet-latest" Mode="" Description="Anthropic model for scheduling. Default: claude-3-5-sonnet-latest" Type="Variable" Display="advanced" Required="false" Mask="false">claude-3-5-sonnet-latest</Config>
  
  <Config Name="Anthropic Reflect Model" Target="ANTHROPIC_MODEL_REFLECT" Default="claude-3-5-sonnet-latest" Mode="" Description="Anthropic model for reflection. Default: claude-3-5-sonnet-latest" Type="Variable" Display="advanced" Required="false" Mask="false">claude-3-5-sonnet-latest</Config>
  
  <!-- Cron Configuration -->
  <Config Name="Cron Schedule" Target="CRON_SCHEDULE" Default="*/5 * * * *" Mode="" Description="How often to check for new emails (cron format). Default: */5 * * * * (every 5 minutes). Examples: */15 * * * * (every 15 min), 0 * * * * (hourly), 0 9-17 * * 1-5 (9am-5pm weekdays)" Type="Variable" Display="always" Required="false" Mask="false">*/5 * * * *</Config>
  
  <Config Name="Minutes Since Last Check" Target="CRON_MINUTES_SINCE" Default="10" Mode="" Description="How many minutes back to look for emails on each cron run. Should be at least equal to your cron frequency. Default: 10" Type="Variable" Display="always" Required="false" Mask="false">10</Config>
  
  <!-- Volume Mappings -->
  <Config Name="Data Directory" Target="/app/data" Default="/mnt/user/appdata/executive-ai-assistant/data" Mode="rw" Description="Persistent data storage for email history and state" Type="Path" Display="always" Required="true" Mask="false">/mnt/user/appdata/executive-ai-assistant/data</Config>
  
  <Config Name="Config Directory" Target="/app/config" Default="/mnt/user/appdata/executive-ai-assistant/config" Mode="rw" Description="Configuration files directory" Type="Path" Display="always" Required="true" Mask="false">/mnt/user/appdata/executive-ai-assistant/config</Config>
  
  <Config Name="Secrets Directory" Target="/app/secrets" Default="/mnt/user/appdata/executive-ai-assistant/secrets" Mode="rw" Description="Secrets storage (alternative to env vars for credentials)" Type="Path" Display="advanced" Required="false" Mask="false">/mnt/user/appdata/executive-ai-assistant/secrets</Config>
  
  <Config Name="Logs Directory" Target="/var/log/eaia" Default="/mnt/user/appdata/executive-ai-assistant/logs" Mode="rw" Description="Application and cron logs for debugging" Type="Path" Display="advanced" Required="false" Mask="false">/mnt/user/appdata/executive-ai-assistant/logs</Config>
  
  <!-- Optional Advanced Configuration -->
  <Config Name="Timezone" Target="TZ" Default="America/New_York" Mode="" Description="Container timezone (affects cron schedule). Examples: America/New_York, America/Chicago, America/Los_Angeles, Europe/London, UTC" Type="Variable" Display="advanced" Required="false" Mask="false">America/New_York</Config>
  
  <!-- OAuth Configuration for Private Networks -->
  <Config Name="Use Desktop App OAuth" Target="USE_LOCALHOST_OAUTH" Default="true" Mode="" Description="OAuth mode: 'true' = Desktop app with localhost redirect (default, works on private networks with SSH port forwarding). 'false' = Web app with public domain redirect (requires reverse proxy and public domain). Most users should leave this as 'true'." Type="Variable" Display="always" Required="false" Mask="false">true</Config>
  
  <!-- Reverse Proxy Configuration -->
  <Config Name="Setup UI Base URL" Target="SETUP_UI_BASE_URL" Default="" Mode="" Description="REVERSE PROXY ONLY: Public URL for OAuth callbacks when behind reverse proxy. Example: https://setup.example.com or https://assistant.example.com/setup. Leave empty for direct access (automatically detected). Required if proxy doesn't forward X-Forwarded-* headers correctly." Type="Variable" Display="advanced" Required="false" Mask="false"/>
  
  <Config Name="Setup UI CORS Origins" Target="CORS_ALLOWED_ORIGINS" Default="*" Mode="" Description="REVERSE PROXY ONLY: Setup UI (port 2025) CORS allowed origins. Comma-separated list. Default: * (all origins). Example: https://setup.example.com,https://app.example.com. Use '*' for development only." Type="Variable" Display="advanced" Required="false" Mask="false">*</Config>
  
  <Config Name="LangGraph API CORS Origins" Target="LANGGRAPH_CORS_ORIGINS" Default="*" Mode="" Description="REVERSE PROXY ONLY: LangGraph API (port 2024) CORS allowed origins. Comma-separated list. Default: * (all origins). Example: https://inbox.example.com,https://agent.example.com. Required if Agent Inbox or other browser-based clients access the API from different domain. Use '*' for development only." Type="Variable" Display="advanced" Required="false" Mask="false">*</Config>
  
  <!-- Monitoring & Debugging -->
  <Config Name="LangSmith API Key" Target="LANGSMITH_API_KEY" Default="" Mode="" Description="Optional: LangSmith API key for tracing and debugging. Get from: https://smith.langchain.com/settings. Leave empty to disable." Type="Variable" Display="advanced" Required="false" Mask="true"/>
  
  <Config Name="LangSmith Tracing" Target="LANGCHAIN_TRACING_V2" Default="false" Mode="" Description="Enable LangSmith tracing (requires LangSmith API Key). Set to 'true' to enable, 'false' to disable." Type="Variable" Display="advanced" Required="false" Mask="false">false</Config>
  
  <Config Name="LangSmith Project" Target="LANGCHAIN_PROJECT" Default="executive-ai-assistant" Mode="" Description="LangSmith project name for organizing traces" Type="Variable" Display="advanced" Required="false" Mask="false">executive-ai-assistant</Config>
  
  <Config Name="Log Level" Target="LOG_LEVEL" Default="INFO" Mode="" Description="Logging verbosity: DEBUG (most detailed), INFO (normal), WARNING (warnings only), ERROR (errors only)" Type="Variable" Display="advanced" Required="false" Mask="false">INFO</Config>
</Container>
