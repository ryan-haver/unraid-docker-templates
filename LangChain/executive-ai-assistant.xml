<?xml version="1.0"?>
<Container version="2">
  <Name>Executive-AI-Assistant</Name>
  <Repository>ghcr.io/ryan-haver/executive-ai-assistant:feature-multi-llm-support</Repository>
  <Registry>https://github.com/ryan-haver/executive-ai-assistant/pkgs/container/executive-ai-assistant</Registry>
  <Network>bridge</Network>
  <MyIP/>
  <Shell>bash</Shell>
  <Privileged>false</Privileged>
  <Support>https://github.com/ryan-haver/executive-ai-assistant</Support>
  <Project>https://github.com/langchain-ai/executive-ai-assistant</Project>
  <Overview>Executive AI Assistant - Your AI-powered email management assistant that drafts professional responses using LangChain and multiple LLM providers.

This intelligent assistant monitors your Gmail inbox, triages emails, drafts professional responses, and helps you manage your communications efficiently using state-of-the-art AI models.

**KEY FEATURES:**
- üìß Automated email monitoring and triage
- ‚úçÔ∏è AI-powered draft response generation
- üîÑ Multi-LLM support (Ollama, OpenAI, Anthropic)
- üéØ Context-aware response generation
- üìÖ Meeting scheduling assistance
- üîç Intelligent email categorization
- üíæ Persistent conversation history
- üîí Secure OAuth2 Gmail integration

**SUPPORTED LLM PROVIDERS:**
- **Ollama** - Free, local AI models (llama3.2, llama3.1, etc.)
- **OpenAI** - GPT-4o, GPT-4o-mini, O1
- **Anthropic** - Claude 3.5 Sonnet, Claude 3 Haiku
- **Hybrid Mode** - Ollama with cloud fallback (recommended)
- **Auto Mode** - Automatically detect available providers

**IMPORTANT - GMAIL OAUTH SETUP (REQUIRED):**

This container monitors your Gmail and drafts responses. It requires one-time OAuth setup:

**ÔøΩ QUICK SETUP (5 MINUTES):**

**Part 1: Google Cloud Console (One-time)**
1. Visit https://console.cloud.google.com/
2. Create new project: "Executive AI Assistant"
3. Enable Gmail API (APIs &amp; Services ‚Üí Enable APIs)
4. Configure OAuth consent screen:
   - User type: **External**
   - App name: "Executive AI Assistant"
   - Add your Gmail as test user
5. Create credentials (APIs &amp; Services ‚Üí Credentials):
   - Type: **Desktop app**
   - Name: "Executive AI Assistant"
   - Download JSON as `client_secret.json`

**Part 2: Container Setup (One-time)**
1. Deploy this container with at least ONE LLM provider configured (see LLM SETUP below)
2. From your PC, open PowerShell and run:
   ```
   ssh -L 2025:localhost:2025 root@YOUR-UNRAID-IP
   ```
   (Replace YOUR-UNRAID-IP with your server's IP like 192.168.1.110)
3. Open http://localhost:2025/setup in your browser
4. Upload `client_secret.json` 
5. Click "Connect to Gmail" and authorize
6. Done! Close PowerShell. Container now runs independently.

**‚ö†Ô∏è SSH tunnel is ONLY for initial setup** - the container runs on its own afterwards, checking email every 5 minutes automatically.

---

**LLM SETUP (REQUIRED - Choose One):**

You must configure at least ONE AI provider:

**Option A: Ollama (Free, Local)**
- Deploy Ollama container from Community Apps
- Run: `docker exec ollama ollama pull llama3.2:3b`
- Set "Ollama Host" to: `http://YOUR-UNRAID-IP:11434`
- Set "LLM Provider" to: `ollama` or `auto`

**Option B: OpenAI (Cloud, Paid)**
- Get API key: https://platform.openai.com/api-keys
- Paste in "OpenAI API Key" field
- Set "LLM Provider" to: `openai` or `auto`

**Option C: Anthropic (Cloud, Paid)**
- Get API key: https://console.anthropic.com/settings/keys
- Paste in "Anthropic API Key" field  
- Set "LLM Provider" to: `anthropic` or `auto`

**üí° Recommended:** Set "LLM Provider" to `auto` - it will use whichever provider you configured.

---

**ACCESSING THE ASSISTANT:**

- **Setup UI:** http://YOUR-UNRAID-IP:2025/setup (for OAuth setup only)
- **LangGraph API:** http://YOUR-UNRAID-IP:2024 (for LangGraph Studio)
- **Agent Inbox UI:** Install separate Agent-Inbox container for web interface

---

**ADVANCED OPTIONS:**

**Custom Models:** In "Show more settings", you can customize which models are used for each task (triage, drafting, etc.)

**Reverse Proxy:** If you have a public domain:
- Set USE_LOCALHOST_OAUTH to `false`
- Use Web app OAuth credentials instead of Desktop app
- Set SETUP_UI_BASE_URL to your public URL

**Troubleshooting:** Check container logs for errors. Most issues are due to missing LLM provider or incorrect OAuth setup.

**Container won't start / Health check failing:**
- Check logs: `docker logs executive-ai-assistant`
- Verify Gmail credentials are base64-encoded correctly
- Ensure at least one LLM provider is configured

**Gmail OAuth errors:**
- Re-run setup_gmail.py script to regenerate token
- Verify OAuth consent screen is configured
- Check that test users include your email
- Make sure Gmail API is enabled

**Ollama connection failed:**
- Verify Ollama is running: `docker ps | grep ollama`
- Test connection: `curl http://YOUR_UNRAID_IP:11434/api/tags`
- Check Ollama Host setting matches your setup
- Ensure network allows container-to-container communication

**No emails being processed:**
- Check cron logs: `docker exec executive-ai-assistant cat /var/log/eaia/cron.log`
- Verify email arrived in last X minutes (CRON_MINUTES_SINCE setting)
- Check Gmail token hasn't expired
- Review API logs for errors

**SECURITY NOTES:**
- Gmail credentials are stored in Docker volumes (not visible in Unraid GUI)
- API keys are masked in the template (not shown in plain text)
- Consider using Ollama for privacy-sensitive emails
- LangSmith tracing is optional (disabled by default)
- Keep your OAuth credentials secure and don't share them

**RESOURCE REQUIREMENTS:**
- CPU: 0.5-2.0 cores (depending on LLM usage)
- RAM: 512MB-2GB (depending on model size)
- Disk: ~1GB for container + email data
- Network: Internet access for cloud LLMs (optional for Ollama-only)

**DOCKER IMAGE DETAILS:**
- Base: python:3.12-slim
- Size: 693MB
- Services: LangGraph API (port 2024) + Setup UI (port 2025) + Cron scheduler
- Health Check: Automatic monitoring of all services

For detailed documentation, visit: https://github.com/ryan-haver/executive-ai-assistant</Overview>
  <Category>Productivity: Tools:</Category>
  <WebUI>http://[IP]:[PORT:2025]/setup</WebUI>
  <TemplateURL>https://raw.githubusercontent.com/ryan-haver/unraid-docker-templates/main/LangChain/executive-ai-assistant.xml</TemplateURL>
  <Icon>https://python.langchain.com/img/brand/wordmark.png</Icon>
  <ExtraParams>--restart=unless-stopped</ExtraParams>
  <PostArgs/>
  <CPUset/>
  <DateInstalled/>
  <DonateText/>
  <DonateLink/>
  <Description>AI-powered email assistant that monitors Gmail, triages emails, and drafts professional responses using LangChain and multiple LLM providers (Ollama, OpenAI, Anthropic).</Description>
  <Networking>
    <Mode>bridge</Mode>
    <Publish>
      <Port>
        <HostPort>2024</HostPort>
        <ContainerPort>2024</ContainerPort>
        <Protocol>tcp</Protocol>
      </Port>
    </Publish>
  </Networking>
  <Data>
    <Volume>
      <HostDir>/mnt/user/appdata/executive-ai-assistant/data</HostDir>
      <ContainerDir>/app/data</ContainerDir>
      <Mode>rw</Mode>
    </Volume>
    <Volume>
      <HostDir>/mnt/user/appdata/executive-ai-assistant/config</HostDir>
      <ContainerDir>/app/config</ContainerDir>
      <Mode>rw</Mode>
    </Volume>
    <Volume>
      <HostDir>/mnt/user/appdata/executive-ai-assistant/secrets</HostDir>
      <ContainerDir>/app/secrets</ContainerDir>
      <Mode>rw</Mode>
    </Volume>
    <Volume>
      <HostDir>/mnt/user/appdata/executive-ai-assistant/logs</HostDir>
      <ContainerDir>/var/log/eaia</ContainerDir>
      <Mode>rw</Mode>
    </Volume>
  </Data>
  <Labels/>
  
  <!-- Required Configuration -->
  <Config Name="LangGraph API Port" Target="2024" Default="2024" Mode="tcp" Description="Port to access the AI assistant API and interact with email processing. Access at: http://YOUR-UNRAID-IP:2024" Type="Port" Display="always" Required="true" Mask="false">2024</Config>
  
  <Config Name="Setup UI Port" Target="2025" Default="2025" Mode="tcp" Description="Port for the Setup Web UI where you configure Gmail OAuth. Access at: http://YOUR-UNRAID-IP:2025/setup during initial setup." Type="Port" Display="always" Required="true" Mask="false">2025</Config>
  
  <Config Name="Gmail Client Secret (Base64)" Target="GMAIL_SECRET" Default="" Mode="" Description="OPTIONAL: Base64-encoded client_secret.json from Google Cloud Console. Most users should leave empty and use the Setup UI (port 2025) instead. To encode manually: cat client_secret.json | base64 -w 0" Type="Variable" Display="advanced" Required="false" Mask="true"/>
  
  <Config Name="Gmail OAuth Token (Base64)" Target="GMAIL_TOKEN" Default="" Mode="" Description="OPTIONAL: Base64-encoded token.json after OAuth authorization. Most users should leave empty and use the Setup UI (port 2025) instead. This is auto-generated after you authorize Gmail access." Type="Variable" Display="advanced" Required="false" Mask="true"/>
  
  <!-- LLM Provider Configuration -->
  <Config Name="LLM Provider" Target="LLM_PROVIDER" Default="auto" Mode="" Description="Which AI service to use: 'auto' (recommended - detects what you have), 'ollama' (free local AI), 'openai' (GPT models, paid), 'anthropic' (Claude models, paid), 'hybrid' (ollama for simple tasks, cloud for complex)." Type="Variable" Display="always" Required="true" Mask="false">auto</Config>
  
  <Config Name="Ollama Host" Target="OLLAMA_HOST" Default="http://192.168.1.113:11434" Mode="" Description="URL to your Ollama server. Examples: http://192.168.1.113:11434 (different server), http://ollama:11434 (Docker container name), http://localhost:11434 (same server)." Type="Variable" Display="always" Required="false" Mask="false">http://192.168.1.113:11434</Config>
  
  <Config Name="OpenAI API Key" Target="OPENAI_API_KEY" Default="" Mode="" Description="Your OpenAI API key to use GPT models. Get yours at: https://platform.openai.com/api-keys (starts with 'sk-proj-'). Leave empty if not using OpenAI." Type="Variable" Display="always" Required="false" Mask="true"/>
  
  <Config Name="Anthropic API Key" Target="ANTHROPIC_API_KEY" Default="" Mode="" Description="Your Anthropic API key to use Claude models. Get yours at: https://console.anthropic.com/settings/keys (starts with 'sk-ant-'). Leave empty if not using Anthropic." Type="Variable" Display="always" Required="false" Mask="true"/>
  
  <!-- Ollama Model Configuration -->
  <Config Name="Ollama Triage Model" Target="OLLAMA_MODEL_TRIAGE" Default="llama3.2:latest" Mode="" Description="Which Ollama model to use for sorting/classifying emails (fast task). Default 'llama3.2:latest' is fast. Try 'llama3.1:8b' for better accuracy. First run: ollama pull llama3.2:latest" Type="Variable" Display="advanced" Required="false" Mask="false">llama3.2:latest</Config>
  
  <Config Name="Ollama Draft Model" Target="OLLAMA_MODEL_DRAFT" Default="llama3.2:latest" Mode="" Description="Which Ollama model to use for writing email responses (quality matters). Default 'llama3.2:latest' is balanced. Try 'llama3.1:70b' for better quality (needs 40GB+ RAM) or 'llama3.2:3b' for speed." Type="Variable" Display="advanced" Required="false" Mask="false">llama3.2:latest</Config>
  
  <Config Name="Ollama Rewrite Model" Target="OLLAMA_MODEL_REWRITE" Default="llama3.2:latest" Mode="" Description="Which Ollama model to use for improving/adjusting tone of drafts. Default 'llama3.2:latest' works well. Try 'llama3.1:70b' for better style adjustments or 'llama3.2:3b' for speed." Type="Variable" Display="advanced" Required="false" Mask="false">llama3.2:latest</Config>
  
  <Config Name="Ollama Schedule Model" Target="OLLAMA_MODEL_SCHEDULE" Default="llama3.2:latest" Mode="" Description="Which Ollama model to use for understanding calendar/meeting requests (reasoning task). Default 'llama3.2:latest' is good. Try 'llama3.1:70b' for complex scheduling logic." Type="Variable" Display="advanced" Required="false" Mask="false">llama3.2:latest</Config>
  
  <Config Name="Ollama Reflect Model" Target="OLLAMA_MODEL_REFLECT" Default="llama3.2:latest" Mode="" Description="Which Ollama model to use for learning from feedback (high-quality reasoning). Default 'llama3.2:latest' is best but needs 40GB+ RAM. Use 'llama3.1:8b' if low on memory." Type="Variable" Display="advanced" Required="false" Mask="false">llama3.2:latest</Config>
  
  <!-- OpenAI Model Configuration -->
  <Config Name="OpenAI Triage Model" Target="OPENAI_MODEL_TRIAGE" Default="gpt-4o-mini" Mode="" Description="Which GPT model to use for email triage (fast, cheap task). Default 'gpt-4o-mini' ($0.15/1M tokens) is perfect. Could use 'gpt-4o' for better accuracy but costs more." Type="Variable" Display="advanced" Required="false" Mask="false">gpt-4o-mini</Config>
  
  <Config Name="OpenAI Draft Model" Target="OPENAI_MODEL_DRAFT" Default="gpt-4o" Mode="" Description="Which GPT model to use for drafting responses (quality matters). Default 'gpt-4o' ($2.50/1M tokens) is high quality. Use 'gpt-4o-mini' to save money or 'o1-mini' for complex logic." Type="Variable" Display="advanced" Required="false" Mask="false">gpt-4o</Config>
  
  <Config Name="OpenAI Rewrite Model" Target="OPENAI_MODEL_REWRITE" Default="gpt-4o" Mode="" Description="Which GPT model to use for rewriting drafts. Default 'gpt-4o' is excellent. Use 'gpt-4o-mini' if cost is a concern." Type="Variable" Display="advanced" Required="false" Mask="false">gpt-4o</Config>
  
  <Config Name="OpenAI Schedule Model" Target="OPENAI_MODEL_SCHEDULE" Default="gpt-4o" Mode="" Description="Which GPT model to use for calendar/scheduling logic. Default 'gpt-4o' is good. Try 'o1' or 'o1-mini' for complex multi-person scheduling (better reasoning, higher cost)." Type="Variable" Display="advanced" Required="false" Mask="false">gpt-4o</Config>
  
  <Config Name="OpenAI Reflect Model" Target="OPENAI_MODEL_REFLECT" Default="o1" Mode="" Description="Which GPT model to use for learning/reflection (best reasoning needed). Default 'o1' ($15/1M tokens) has best reasoning. Use 'o1-mini' ($3/1M) or 'gpt-4o' to save money." Type="Variable" Display="advanced" Required="false" Mask="false">o1</Config>
  
  <!-- Anthropic Model Configuration -->
  <Config Name="Anthropic Triage Model" Target="ANTHROPIC_MODEL_TRIAGE" Default="claude-3-haiku-20240307" Mode="" Description="Which Claude model to use for email triage (fast, cheap task). Default 'claude-3-haiku' ($0.25/1M tokens) is perfect. Use 'claude-3-5-sonnet-latest' for better accuracy." Type="Variable" Display="advanced" Required="false" Mask="false">claude-3-haiku-20240307</Config>
  
  <Config Name="Anthropic Draft Model" Target="ANTHROPIC_MODEL_DRAFT" Default="claude-3-5-sonnet-latest" Mode="" Description="Which Claude model to use for drafting responses (quality matters). Default 'claude-3-5-sonnet-latest' ($3/1M tokens) has excellent writing quality. Use 'claude-3-haiku' to save money." Type="Variable" Display="advanced" Required="false" Mask="false">claude-3-5-sonnet-latest</Config>
  
  <Config Name="Anthropic Rewrite Model" Target="ANTHROPIC_MODEL_REWRITE" Default="claude-3-5-sonnet-latest" Mode="" Description="Which Claude model to use for rewriting drafts. Default 'claude-3-5-sonnet-latest' excels at tone adjustment. Use 'claude-3-haiku' to save money." Type="Variable" Display="advanced" Required="false" Mask="false">claude-3-5-sonnet-latest</Config>
  
  <Config Name="Anthropic Schedule Model" Target="ANTHROPIC_MODEL_SCHEDULE" Default="claude-3-5-sonnet-latest" Mode="" Description="Which Claude model to use for calendar/scheduling. Default 'claude-3-5-sonnet-latest' has strong reasoning for complex scheduling." Type="Variable" Display="advanced" Required="false" Mask="false">claude-3-5-sonnet-latest</Config>
  
  <Config Name="Anthropic Reflect Model" Target="ANTHROPIC_MODEL_REFLECT" Default="claude-3-5-sonnet-latest" Mode="" Description="Which Claude model to use for learning/reflection. Default 'claude-3-5-sonnet-latest' provides thoughtful analysis and improvement suggestions." Type="Variable" Display="advanced" Required="false" Mask="false">claude-3-5-sonnet-latest</Config>
  
  <!-- Cron Configuration -->
  <Config Name="Cron Schedule" Target="CRON_SCHEDULE" Default="*/5 * * * *" Mode="" Description="How often to automatically check Gmail for new emails (cron format). Default: */5 * * * * = every 5 minutes. Examples: */15 * * * * = every 15 min, 0 * * * * = hourly, 0 9-17 * * 1-5 = weekdays 9am-5pm only." Type="Variable" Display="always" Required="false" Mask="false">*/5 * * * *</Config>
  
  <Config Name="Minutes Since Last Check" Target="CRON_MINUTES_SINCE" Default="10" Mode="" Description="How far back (in minutes) to look for emails on each check. Should be slightly longer than your cron frequency to avoid gaps. Example: if checking every 5 minutes, use 10 to have overlap." Type="Variable" Display="always" Required="false" Mask="false">10</Config>
  
  <!-- Volume Mappings -->
  <Config Name="Data Directory" Target="/app/data" Default="/mnt/user/appdata/executive-ai-assistant/data" Mode="rw" Description="Where to store email history, AI state, and processed messages. This persists across container restarts. Default location works for most Unraid setups." Type="Path" Display="always" Required="true" Mask="false">/mnt/user/appdata/executive-ai-assistant/data</Config>
  
  <Config Name="Config Directory" Target="/app/config" Default="/mnt/user/appdata/executive-ai-assistant/config" Mode="rw" Description="Where to store configuration files. This persists your settings across container restarts. Default location works for most Unraid setups." Type="Path" Display="always" Required="true" Mask="false">/mnt/user/appdata/executive-ai-assistant/config</Config>
  
  <Config Name="Secrets Directory" Target="/app/secrets" Default="/mnt/user/appdata/executive-ai-assistant/secrets" Mode="rw" Description="ADVANCED: Alternative location for storing API keys and credentials as files instead of environment variables. Leave default unless you prefer file-based secrets management." Type="Path" Display="advanced" Required="false" Mask="false">/mnt/user/appdata/executive-ai-assistant/secrets</Config>
  
  <Config Name="Logs Directory" Target="/var/log/eaia" Default="/mnt/user/appdata/executive-ai-assistant/logs" Mode="rw" Description="Where to store application logs for troubleshooting. Logs include cron runs, API calls, and errors. Useful for debugging issues." Type="Path" Display="advanced" Required="false" Mask="false">/mnt/user/appdata/executive-ai-assistant/logs</Config>
  
  <!-- Optional Advanced Configuration -->
  <Config Name="Timezone" Target="TZ" Default="America/Denver" Mode="" Description="Your timezone - affects when cron jobs run. Examples: America/New_York (EST/EDT), America/Chicago (CST/CDT), America/Los_Angeles (PST/PDT), America/Denver (MST/MDT), Europe/London, Asia/Tokyo, UTC. Find yours at: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones" Type="Variable" Display="advanced" Required="false" Mask="false">America/Denver</Config>
  
  <!-- OAuth Configuration for Private Networks -->
  <Config Name="Use Desktop App OAuth" Target="USE_LOCALHOST_OAUTH" Default="true" Mode="" Description="OAuth setup mode. 'true' (RECOMMENDED) = Desktop app type with localhost redirect - works on private networks using SSH port forwarding during setup. 'false' = Web app type - requires public domain and reverse proxy. Most users should keep this as 'true'." Type="Variable" Display="always" Required="false" Mask="false">true</Config>
  
  <!-- Reverse Proxy Configuration -->
  <Config Name="Setup UI Base URL" Target="SETUP_UI_BASE_URL" Default="" Mode="" Description="REVERSE PROXY ONLY: Your public URL for OAuth callbacks when behind a reverse proxy. Examples: https://setup.yourdomain.com or https://assistant.yourdomain.com/setup. Leave EMPTY for direct access (container auto-detects the URL). Only needed if your reverse proxy doesn't forward X-Forwarded-Host/X-Forwarded-Proto headers correctly." Type="Variable" Display="advanced" Required="false" Mask="false"/>
  
  <Config Name="Setup UI CORS Origins" Target="CORS_ALLOWED_ORIGINS" Default="*" Mode="" Description="REVERSE PROXY ONLY: Which web addresses can access the Setup UI (port 2025). Use comma-separated list for multiple. Examples: https://setup.yourdomain.com,https://app.yourdomain.com. Default '*' allows all (fine for private networks, but change for public access)." Type="Variable" Display="advanced" Required="false" Mask="false">*</Config>
  
  <Config Name="LangGraph API CORS Origins" Target="LANGGRAPH_CORS_ORIGINS" Default="*" Mode="" Description="REVERSE PROXY ONLY: Which web addresses can access the LangGraph API (port 2024). Needed if Agent Inbox or other browser apps access from a different domain. Examples: https://inbox.yourdomain.com,https://agent.yourdomain.com. Default '*' allows all (fine for private networks)." Type="Variable" Display="advanced" Required="false" Mask="false">*</Config>
  
  <!-- Monitoring & Debugging -->
  <Config Name="LangSmith API Key" Target="LANGSMITH_API_KEY" Default="" Mode="" Description="OPTIONAL: Your LangSmith API key for advanced tracing and debugging of AI interactions. Get free account at: https://smith.langchain.com/settings (useful for seeing exactly what the AI is doing). Leave empty to skip." Type="Variable" Display="advanced" Required="false" Mask="true"/>
  
  <Config Name="LangSmith Tracing" Target="LANGCHAIN_TRACING_V2" Default="false" Mode="" Description="Enable LangSmith tracing? Requires LangSmith API Key above. 'true' = send AI traces to LangSmith for debugging, 'false' = disabled (saves API calls). Only useful if you're troubleshooting AI behavior." Type="Variable" Display="advanced" Required="false" Mask="false">false</Config>
  
  <Config Name="LangSmith Project" Target="LANGCHAIN_PROJECT" Default="executive-ai-assistant" Mode="" Description="LangSmith project name to organize your traces under (if using LangSmith). Default 'executive-ai-assistant' works fine. Change if you want to separate multiple deployments." Type="Variable" Display="advanced" Required="false" Mask="false">executive-ai-assistant</Config>
  
  <Config Name="Log Level" Target="LOG_LEVEL" Default="INFO" Mode="" Description="How much detail in the logs? 'DEBUG' = everything (very detailed, use for troubleshooting), 'INFO' = normal operation details (recommended), 'WARNING' = only warnings and errors, 'ERROR' = only errors. Check logs at: /mnt/user/appdata/executive-ai-assistant/logs/" Type="Variable" Display="advanced" Required="false" Mask="false">INFO</Config>
</Container>
